{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "f4febdd1bdd910197937d794d7fbd716", "grade": false, "grade_id": "cell-570cf80ae1b2c48e", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# Actividad 1: Pipeline de procesamiento de datos con HDFS y Spark\n\n## Actividad del Proyecto (actividad grupal)\n\nEsta actividad est\u00e1 asociada al Proyecto transversal del t\u00edtulo y para su desarrollo, tendr\u00e1s que utilizar obligatoriamente el siguiente recurso:\n* Dataset flights del Cat\u00e1logo de Datos del proyecto"}, {"cell_type": "markdown", "metadata": {}, "source": "## Escribe aqu\u00ed los nombres de los integrantes del grupo: Daniel Jim\u00e9nez Alonso, Pedro Dur\u00e1n Porras, Juan Carlos Regueiro Garc\u00eda, Juan Miguel Moreno Valero\n\n## Recuerda borrar siempre las l\u00edneas que dicen `raise NotImplementedError`"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "7a238d899b5a6e8c414330ade880233f", "grade": false, "grade_id": "cell-f4c598b6fd61ee12", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Lee con detenimiento cada ejercicio. Las variables utilizadas para almacenar las soluciones, al igual que las nuevas columnas creadas, deben llamarse **exactamente** como indica el ejercicio, o de lo contrario los tests fallar\u00e1n y el ejercicio no puntuar\u00e1. Debe reemplazarse el valor `None` al que est\u00e1n inicializadas por el c\u00f3digo necesario para resolver el ejercicio."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "db4faded3e277c6e6253d9a9ba99f2d9", "grade": false, "grade_id": "cell-42368b0202b6ce77", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "## Leemos el fichero flights.csv que hemos subido a Google Cloud Storage"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "219cb0eba4e188af1ef349ee35d7d334", "grade": false, "grade_id": "cell-3202a483f423590a", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Aunque para las capturas de pantalla se pide subir el fichero a HDFS, el resto de la actividad puede hacerse leyendo el mismo fichero que hemos subido al bucket de Google Cloud Storage.\n\nIndicamos que contiene encabezados (nombres de columnas) y que intente inferir el esquema, aunque despu\u00e9s comprobaremos si lo\nha inferido correctamente o no."}, {"cell_type": "code", "execution_count": 1, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "50a7e93a1c2653ab52ccfbaf70c0edbb", "grade": false, "grade_id": "lectura-fichero", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "ruta_gcs = \"/daniel_jimenez_alonso/flights.csv\" # Reemplaza esto por la ruta correcta del fichero flights.csv en tu bucket de Google Cloud Storage\nflightsDF = \"gs://danieljimenezalonsounir_actividad1/actividad1/flights.csv\"\n\n# Descomentar estas l\u00edneas\nflightsDF = spark.read\\\n            .option(\"header\", \"true\")\\\n            .option(\"inferSchema\", \"true\")\\\n            .csv(ruta_gcs)\n\n# YOUR CODE HERE\n#raise NotImplementedError"}, {"cell_type": "markdown", "metadata": {}, "source": "Imprimimos el esquema para comprobar qu\u00e9 tipo de dato ha inferido en cada columna"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- dep_time: string (nullable = true)\n |-- dep_delay: string (nullable = true)\n |-- arr_time: string (nullable = true)\n |-- arr_delay: string (nullable = true)\n |-- carrier: string (nullable = true)\n |-- tailnum: string (nullable = true)\n |-- flight: integer (nullable = true)\n |-- origin: string (nullable = true)\n |-- dest: string (nullable = true)\n |-- air_time: string (nullable = true)\n |-- distance: integer (nullable = true)\n |-- hour: string (nullable = true)\n |-- minute: string (nullable = true)\n\n"}], "source": "flightsDF.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Mostramos el n\u00famero de filas que tiene el DataFrame para hacernos una idea de su tama\u00f1o:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "162049"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "flightsDF.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "Vemos que tenemos 162049 filas. Si imprimimos por pantalla las 5 primeras filas, veremos qu\u00e9 tipos parecen tener y en qu\u00e9 columnas no coincide el tipo que podr\u00edamos esperar con el tipo que ha inferido Spark."}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n|2014|    1|  1|       1|       96|     235|       70|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n|2014|    1|  1|       4|       -6|     738|      -23|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n|2014|    1|  1|       8|       13|     548|       -4|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n|2014|    1|  1|      28|       -2|     800|      -23|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n|2014|    1|  1|      34|       44|     325|       43|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\nonly showing top 5 rows\n\n"}], "source": "flightsDF.show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "La causa del problema es que en muchas columnas existe un valor faltante llamado \"NA\". Spark no reconoce ese valor como\n*no disponible* ni nada similar, sino que lo considera como un string de texto normal, y por tanto, asigna a toda la columna\nel tipo de dato string (cadena de caracteres). Concretamente, las siguientes columnas deber\u00edan ser de tipo entero pero Spark\nlas muestra como string:\n<ul>\n <li>dep_time: string (nullable = true)\n <li>dep_delay: string (nullable = true)\n <li>arr_time: string (nullable = true)\n <li>arr_delay: string (nullable = true)\n <li>air_time: string (nullable = true)\n <li>hour: string (nullable = true)\n <li>minute: string (nullable = true)    \n</ul>\n"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a averiguar cu\u00e1ntas filas tienen el valor \"NA\" (como string) en la columna dep_time:"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "857"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import functions as F\ncuantos_NA = flightsDF\\\n                .where(F.col(\"dep_time\") == \"NA\")\\\n                .count()\ncuantos_NA"}, {"cell_type": "markdown", "metadata": {}, "source": "Por tanto, hay 857 filas que no tienen un dato v\u00e1lido en esa columna. Hay distintas maneras de trabajar con los valores faltantes, como por ejemplo imputarlos (reemplazarlos por un valor generado por nosotros seg\u00fan cierta l\u00f3gica, por ejemplo la media de esa columna, etc). Lo m\u00e1s sencillo es quitar toda la fila, aunque esto depende de si nos lo podemos permitir en base\na la cantidad de datos que tenemos. En nuestro caso, como tenemos un n\u00famero considerable de filas, vamos a quitar todas las filas donde hay un NA en cualquiera de las columnas."}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[year: int, month: int, day: int, dep_time: string, dep_delay: string, arr_time: string, arr_delay: string, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: string, distance: int, hour: string, minute: string]"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "columnas_limpiar = [\"dep_time\", \"dep_delay\", \"arr_time\", \"arr_delay\", \"air_time\", \"hour\", \"minute\"]\n\nflightsLimpiado = flightsDF\nfor nombreColumna in columnas_limpiar:  # para cada columna, nos quedamos con las filas que no tienen NA en esa columna\n    flightsLimpiado = flightsLimpiado.where(F.col(nombreColumna) != \"NA\")\n\nflightsLimpiado.cache()"}, {"cell_type": "markdown", "metadata": {}, "source": "Si ahora mostramos el n\u00famero de filas que tiene el DataFrame `flightsLimpiado` tras eliminar todas esas filas, vemos que ha disminuido ligeramente\npero sigue siendo un n\u00famero considerable como para realizar anal\u00edtica y sacar conclusiones sobre estos datos"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "160748"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "flightsLimpiado.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "Una vez que hemos eliminado los NA, vamos a convertir a tipo entero cada una de esas columnas que eran de tipo string. \nAhora no debe haber problema ya que todas las cadenas de texto contienen dentro un n\u00famero que puede ser convertido de texto a n\u00famero. Vamos tambi\u00e9n a convertir la columna `arr_delay` de tipo entero a n\u00famero real, necesario para los pasos posteriores donde ajustaremos un modelo predictivo."}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: double, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]"}, "execution_count": 8, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.types import IntegerType, DoubleType\n\nflightsConvertido = flightsLimpiado\n\nfor c in columnas_limpiar:\n    # m\u00e9todo que crea una columna o reemplaza una existente\n    flightsConvertido = flightsConvertido.withColumn(c, F.col(c).cast(IntegerType())) \n\nflightsConvertido = flightsConvertido.withColumn(\"arr_delay\", F.col(\"arr_delay\").cast(DoubleType()))\nflightsConvertido.cache()"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- dep_time: integer (nullable = true)\n |-- dep_delay: integer (nullable = true)\n |-- arr_time: integer (nullable = true)\n |-- arr_delay: double (nullable = true)\n |-- carrier: string (nullable = true)\n |-- tailnum: string (nullable = true)\n |-- flight: integer (nullable = true)\n |-- origin: string (nullable = true)\n |-- dest: string (nullable = true)\n |-- air_time: integer (nullable = true)\n |-- distance: integer (nullable = true)\n |-- hour: integer (nullable = true)\n |-- minute: integer (nullable = true)\n\n"}], "source": "flightsConvertido.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a volver a mostrar las 5 primeras filas del DataFrame limpio. Aparentemente son iguales a las que ya ten\u00edamos, pero ahora\nSpark s\u00ed est\u00e1 tratando como enteros las columnas que deber\u00edan serlo, y si queremos podemos hacer operaciones aritm\u00e9ticas\ncon ellas."}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 12:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n|2014|    1|  1|       1|       96|     235|     70.0|     AS| N508AS|   145|   PDX| ANC|     194|    1542|   0|     1|\n|2014|    1|  1|       4|       -6|     738|    -23.0|     US| N195UW|  1830|   SEA| CLT|     252|    2279|   0|     4|\n|2014|    1|  1|       8|       13|     548|     -4.0|     UA| N37422|  1609|   PDX| IAH|     201|    1825|   0|     8|\n|2014|    1|  1|      28|       -2|     800|    -23.0|     US| N547UW|   466|   PDX| CLT|     251|    2282|   0|    28|\n|2014|    1|  1|      34|       44|     325|     43.0|     AS| N762AS|   121|   SEA| ANC|     201|    1448|   0|    34|\n+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "flightsConvertido.show(5)"}, {"cell_type": "code", "execution_count": 11, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 13:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+-----+\n|carrier|count|\n+-------+-----+\n|     UA|16452|\n|     AA| 7474|\n|     B6| 3493|\n|     DL|16637|\n|     OO|18368|\n|     F9| 2683|\n|     US| 5876|\n|     HA| 1092|\n|     AS|62189|\n|     VX| 3266|\n|     WN|23218|\n+-------+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "flightsConvertido.groupBy(\"carrier\").count().show()"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "482e3795c3962a4780c282594741996b", "grade": false, "grade_id": "cell-c0cfdd1db1edaa7d", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Ejercicio 1 (3 puntos) \n### (s\u00f3lo punt\u00faa 3 puntos si pasa la celda de autoevaluaci\u00f3n sin errores, o 0 puntos si hay alg\u00fan error. No hay puntaje intermedio)\n\nPartiendo del DataFrame `flightsConvertido` que ya tiene los tipos correctos en las columnas, se pide **encadenar transformaciones sin crear ninguna variable intermedia**, de manera que se vayan creando las siguientes columnas:\n\n* Una nueva columna llamada `flight_date` con la fecha del vuelo como objeto fecha. Utiliza para ello la funci\u00f3n `F.make_date` aplicada a los argumentos (columnas) `year`, `month` y `day` tal como indica la documentaci\u00f3n de `make_date` disponible [aqu\u00ed](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.make_date.html).\n* Encadenado con la transformaci\u00f3n anterior, crear una nueva columna llamada `day_of_week` con el d\u00eda de la semana correspondiente al vuelo, utilizando la funci\u00f3n `F.dayofweek` cuya documentaci\u00f3n puedes ver [aqu\u00ed](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.dayofweek.html). Debes pasarle como argumento la columna `flight_date` creada en el apartado anterior.\n* Encadenado con la transformaci\u00f3n anterior, agrupar para obtener **tantas filas como aeropuertos de destino existan, y tantas columnas como d\u00edas de la semana** hay en el DF original. \n  * Cada celda del DF resultante debe contener el *conteo* del n\u00famero de vuelos que existen para ese destino en ese d\u00eda de la semana.\n  * El DF resultante de la agrupaci\u00f3n seguida de pivot tendr\u00e1 8 columnas, que ser\u00e1n el destino, y los 7 d\u00edas de la semana, llamados del 1 al 7.\n* Encadenado con la transformaci\u00f3n anterior, rellenar los valores nulos con un 0, puesto que todos los valores representan conteos, y los nulos han sido provocados porque ciertas combinaciones de (origen, destino, d\u00eda de la semana) no existen en los datos (no hay vuelos para ciertas rutas en ciertos d\u00edas de la semana). Por eso, tiene sentido sustituir los nulos por 0, que es el conteo para esa combinaci\u00f3n no encontrada.\n* Encadenado con la transformaci\u00f3n anterior, renombrar las columnas de los d\u00edas de la semana para que pasen a llamarse Domingo, Lunes, ..., Sabado (sin tildes). Puedes hacerlo encadenando `withColumnRenamed` varias veces, o bien con `selectExpr` utilizando `as` en cada argumento de tipo string. Consulta la documentaci\u00f3n de `selectExpr` [aqu\u00ed](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.selectExpr.html).\n  * Si utilizas selectExpr, entonces debes referirte a las columnas como ``\"`1`\"`` (con acento grave) y no como `\"1\"` ya que, al tener por nombre un n\u00famero entero, hay que referirse a ellas de ese modo. Si utilizas withColumnRenamed, no es necesario y puedes referirte a ellas como `\"1\"`.\n  * Si lo deseas, puedes recategorizar la columna day_of_week, justo despu\u00e9s de crearla, utilizando para ello otra llamada a `withColumn` para reemplazar dicha columna, usando `F.when` dentro de ella. De este modo, al llegar al groupBy(...).pivot(...) ya tendr\u00e1 los nombres de columna correctos y no ser\u00e1 necesario renombrar columnas.\n* Encadenado con la transformaci\u00f3n anterior, a\u00f1adir una nueva columna llamada `n_vuelos` que contenga, en cada fila, la suma de las columnas de los 7 d\u00edas de la semana. Esto representa el n\u00famero de vuelos totales para ese destino, independientemente del d\u00eda del vuelo.\n* Encadenado con la transformaci\u00f3n anterior, generar un nuevo DF que est\u00e9 ordenado por la columna `n_vuelos` *descendentemente*.\n* El DF resultante de todas estas transformaciones, que deben encadenarse sin utilizar ninguna otra variable, debe quedar guardado en la variable `conteos_df`.\n* No est\u00e1 permitido utilizar construcciones de la forma df = df.algo(). **S\u00f3lo puede utilizarse una vez el operador de asignaci\u00f3n (el signo `=`).**"}, {"cell_type": "code", "execution_count": 12, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "e7ea4e8f96e75f00e8c2ef6c0668f500", "grade": false, "grade_id": "ejercicio-1", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 16:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-------+-----+------+---------+------+-------+------+--------+\n|dest|Domingo|Lunes|Martes|Miercoles|Jueves|Viernes|Sabado|n_vuelos|\n+----+-------+-----+------+---------+------+-------+------+--------+\n| SFO|   1723| 1922|  1840|     1863|  1917|   1910|  1471|   12646|\n| LAX|   1473| 1550|  1502|     1541|  1550|   1545|  1248|   10409|\n| DEN|   1338| 1423|  1348|     1385|  1411|   1376|  1152|    9433|\n| PHX|   1211| 1259|  1203|     1272|  1260|   1254|  1142|    8601|\n| LAS|   1128| 1205|  1166|     1176|  1200|   1194|  1113|    8182|\n+----+-------+-----+------+---------+------+-------+------+--------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import functions as F\n# Reemplaza None por el c\u00f3digo necesario para calcular sus valores correctos\nconteos_df = flightsConvertido\\\n            .withColumn('flight_date', F.make_date(\"year\", \"month\", \"day\"))\\\n            .withColumn('day_of_week', F.dayofweek('flight_date'))\\\n            .groupBy('dest')\\\n            .pivot('day_of_week', [1,2,3,4,5,6,7])\\\n            .agg(F.count('*').alias('count'))\\\n            .selectExpr(\"dest\", \"`1` as Domingo\",\"`2` as Lunes\",\"`3` as Martes\",\"`4` as Miercoles\",\"`5` as Jueves\", \"`6` as Viernes\",\"`7` as Sabado\")\\\n            .fillna(0)\\\n            .withColumn('n_vuelos', sum(F.col(day) for day in [\"Domingo\", \"Lunes\", \"Martes\", \"Miercoles\", \"Jueves\", \"Viernes\", \"Sabado\"]))\\\n            .orderBy(F.col('n_vuelos').desc())\\\n            \n#Comprobacion tabla\nconteos_df.show(5)\n\n\n# YOUR CODE HERE\n#raise NotImplementedError"}, {"cell_type": "code", "execution_count": 13, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "4e63cd0d4d85442a33c40bb3d6b9cd16", "grade": true, "grade_id": "ejercicio-1-test", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "from pyspark.sql import functions as F\nc = conteos_df.columns\nassert(len(c) == 9)  # debe tener 9 columnas que son dest, los d\u00edas de la semana, y n_vuelos (no necesariamente en ese orden)\nassert(all([x in c for x in [\"dest\", \"n_vuelos\", \"Domingo\", \"Lunes\", \"Martes\", \"Miercoles\", \"Jueves\", \"Viernes\", \"Sabado\"]]))  # Comprobar nombres de columnas\ncnt_list = conteos_df.take(100)\nassert(cnt_list[0].dest == \"SFO\" and cnt_list[0].Viernes == 1910)\nassert(cnt_list[1].dest == \"LAX\" and cnt_list[1].Lunes == 1550)\nassert(cnt_list[2].dest == \"DEN\" and cnt_list[2].n_vuelos == 9433)\nassert(cnt_list[-1].Domingo == 0 and cnt_list[-1].n_vuelos == 2)  # la ruta con menos vuelos de todos. Nos aseguramos de que se hayan rellenado los nulos con 0."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "003daa21c5598342ea0689e753b37f86", "grade": false, "grade_id": "cell-2b5f0dea18728fcf", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Ejercicio 2 (3 puntos)\n### (s\u00f3lo punt\u00faa 3 puntos si pasa la celda de autoevaluaci\u00f3n sin errores, o 0 puntos si hay alg\u00fan error. No hay puntaje intermedio)\n\nPartiendo de nuevo de `flightsConvertido`, se pide crear una funci\u00f3n `retraso_medio_periodo`, que reciba como argumento un DF en el podemos asumir que existen las columnas `dep_time` y `arr_delay` y haga lo siguiente:\n\n* A\u00f1ada una nueva columna `periodo_dia` para recategorizar `dep_time` con los siguientes valores sin tildes (puedes hacerlo con withColumn y F.when):\n  * `\"maniana\"` cuando la hora de salida del vuelo est\u00e9 entre las 7 de la ma\u00f1ana no incluida (expresado como 700 en dep_time) y las 12 de la ma\u00f1ana incluida (expresado como 1200)\n  * `\"mediodia\"` cuando la hora de salida del vuelo est\u00e9 entre las 12 de la ma\u00f1ana no incluida (expresado como 1200) y las 5 de la tarde incluida (expresado como 1700)\n  * `\"tarde\"` cuando la hora de salida del vuelo est\u00e9 entre las 5 de la tarde no incluida (expresado como 1700) y las 9 de la noche incluida (expresado como 2100)\n  * `\"noche\"` cuando la hora de salida del vuelo est\u00e9 entre las 9 de la noche no incluida (expresado como 2100) y las 7 de la ma\u00f1ana incluida (expresado como 700)\n  * PISTAS:\n    * Recuerda que las condiciones compuestas con columnas booleanas en Spark requieren obligatoriamente utilizar par\u00e9ntesis envolviendo a cada condici\u00f3n simple.\n    * Recuerda tambi\u00e9n utilizar F.lit(...) para indicar el valor (constante) de la nueva columna para cada una de estas condiciones.\n    * Cuidado con la noche: la condici\u00f3n es que la hora sea mayor que 2100 **o bien** (condici\u00f3n `|`) menor o igual que 700. En el resto de condiciones, necesitamos `&`.\n\n* Encadenado con la transformaci\u00f3n anterior, s\u00f3lo para los vuelos que llegan con **retraso estrictamente positivo (>0)**, calcular el retraso medio a la llegada de dichos vuelos **para cada aeropuerto de destino y cada hora del d\u00eda**, desplegando las franjas horarias como columnas independientes. \n  * El DF calculado debe tener tantas columnas como franjas horarias m\u00e1s la columna del aeropuerto de destino (es decir, 5 columnas en total)\n\n* Encadenado con la transformaci\u00f3n anterior, tras haber hecho la agrupaci\u00f3n y agregaci\u00f3n, a\u00f1adir una columna constante (con `F.lit`) llamada `alumno` que contenga el primer apellido de cada miembro del grupo (ejemplo: si el grupo lo forman Pablo Garc\u00eda, Francisco P\u00e9rez y Antonio Mart\u00edn, el valor de la columna debe ser `Garc\u00eda_P\u00e9rez_Mart\u00edn`).\n* Encadenado con la transformaci\u00f3n anterior, ordenar alfab\u00e9ticamente el DF resultante por aeropuerto de destino, ascendentemente. El resultado de la ordenaci\u00f3n ser\u00e1 un nuevo DF que debe ser devuelto como resultado de la funci\u00f3n (recuerda que es imposible modificar el DF original)."}, {"cell_type": "code", "execution_count": 14, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "cefef7decefd27666b4b92bdcea8a45e", "grade": false, "grade_id": "ejercicio-2", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 28:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+------------------+------------------+-----------------+------------------+--------------------+\n|dest|           maniana|          mediodia|            tarde|             noche|              alumno|\n+----+------------------+------------------+-----------------+------------------+--------------------+\n| ABQ|12.912280701754385|23.872340425531913|             99.4| 8.333333333333334|Jimenez_Regueiro_...|\n| ANC|15.314893617021276|16.876840696117803|18.20250521920668|20.703105590062112|Jimenez_Regueiro_...|\n| ATL|19.243291592128802|             28.75|            375.0|18.298200514138816|Jimenez_Regueiro_...|\n+----+------------------+------------------+-----------------+------------------+--------------------+\nonly showing top 3 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import DataFrame\n\ndef retraso_medio_periodo(df: DataFrame) -> DataFrame:  # El argumento que recibe la funci\u00f3n es un DataFrame de Spark, y el resultado devuelto tambi\u00e9n\n    df = df.withColumn(\"periodo_dia\", \n            F.when((F.col(\"dep_time\") > 700) & (F.col(\"dep_time\") <= 1200), F.lit(\"maniana\"))\n             .when((F.col(\"dep_time\") > 1200) & (F.col(\"dep_time\") <= 1700), F.lit(\"mediodia\"))\n             .when((F.col(\"dep_time\") > 1700) & (F.col(\"dep_time\") <= 2100), F.lit(\"tarde\"))\n             .when((F.col(\"dep_time\") > 2100) | (F.col(\"dep_time\") <= 700), F.lit(\"noche\")))\\\n           .filter(F.col(\"arr_delay\") > 0)\\\n           .groupBy(\"dest\")\\\n           .pivot(\"periodo_dia\", [\"maniana\", \"mediodia\", \"tarde\", \"noche\"])\\\n           .agg(F.avg(\"arr_delay\").alias(\"retraso_medio\"))\\\n           .fillna(0)\\\n           .withColumn(\"alumno\", F.lit(\"Jimenez_Regueiro_Moreno_Duran\"))\\\n           .orderBy(F.col('dest').asc())    \n    return df  # Reemplaza None por la variable que quieras devolver\n\n#COMPROBACIONES\nretraso = retraso_medio_periodo(flightsConvertido)\nretraso.show(3)\n\n\n\n\n# YOUR CODE HERE\n#raise NotImplementedError"}, {"cell_type": "code", "execution_count": 15, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "4309c9fd2d3c277d4fb49ab33a609e3f", "grade": true, "grade_id": "ejercicio-2-tests", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "retraso_medio_df = retraso_medio_periodo(flightsConvertido)\nlista = retraso_medio_df.take(3)\nc = retraso_medio_df.columns\nassert(len(c) == 6)  #  el DF resultante debe tener 6 columnas\nassert(all([x in c for x in [\"dest\", \"maniana\", \"mediodia\", \"tarde\", \"noche\", \"alumno\"]]))  # Comprobamos los nombres de columnas\nassert(lista[0].dest == \"ABQ\" and round(lista[0].maniana, 2) == 12.91 and round(lista[0].noche, 2) == 8.33)\nassert(lista[1].dest == \"ANC\" and round(lista[1].mediodia, 2) == 16.88 and round(lista[1].noche, 2) == 20.70)\nassert(lista[2].dest == \"ATL\" and round(lista[2].tarde, 2) == 375.0 and round(lista[2].noche, 2) == 18.30)"}, {"cell_type": "markdown", "metadata": {}, "source": "Ahora invocamos a nuestra funci\u00f3n `retrasoMedio` pas\u00e1ndole como argumento `flightsConvertido`. \u00bfCu\u00e1les son los tres aeropuertos con mayor retraso medio? \u00bfCu\u00e1les son sus retrasos medios en minutos?"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "fa9ed55cd86eb0958e150d6a918db1af", "grade": false, "grade_id": "cell-e577747d4427e32b", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Ejercicio 3\n\nAjustar un modelo de DecisionTree de Spark para predecir si un vuelo vendr\u00e1 o no con retraso (problema de clasificaci\u00f3n binaria), utilizando como variables predictoras el mes, el d\u00eda del mes, la hora de partida `dep_time`, la hora de llegada `arr_time`, el tipo de avi\u00f3n (`carrier`), la distancia y el tiempo que permanece en el aire. Para ello, sigue los siguientes pasos."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "bd5d9c35150d9184f07b369c29a44789", "grade": false, "grade_id": "cell-e577747d4427e32a", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Notemos que en estos datos hay variables num\u00e9ricas y variables categ\u00f3ricas que ahora mismo est\u00e1n tipadas como num\u00e9ricas, como por ejemplo el mes del a\u00f1o (`month`), que es en realidad categ\u00f3rica. Debemos indicar a Spark cu\u00e1les son categ\u00f3ricas e indexarlas. Para ello se pide: \n\n* **(1 punto)** Crear un `StringIndexer` llamado `indexerMonth` y otro llamado `indexerCarrier` sobre las variables categ\u00f3ricas `month` y `carrier` (tipo de avi\u00f3n). El nombre de las columnas indexadas que se crear\u00e1n debe ser, respectivamente, `monthIndexed` y `carrierIndexed`. Aseg\u00farate de que, cuando necesite codificar una categor\u00eda que no exist\u00eda cuando se entren\u00f3, esta pieza no lance un error sino que le asigne el primer valor que est\u00e9 libre (argumento `handleInvalid`)."}, {"cell_type": "code", "execution_count": 16, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "eb039584cd14e6bc3434e5be930341e6", "grade": false, "grade_id": "string-indexer", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# Incluye aqu\u00ed los imports que necesites\nfrom pyspark.ml.feature import StringIndexer\n\nindexerMonth = StringIndexer(inputCol=\"month\", outputCol=\"monthIndexed\",handleInvalid=\"keep\")\nindexerCarrier = StringIndexer(inputCol=\"carrier\", outputCol=\"carrierIndexed\",handleInvalid=\"keep\")\n\n# YOUR CODE HERE\n#raise NotImplementedError"}, {"cell_type": "code", "execution_count": 17, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "eb2fa0df13e5dcf2121097b75e7c5dfe", "grade": true, "grade_id": "string-indexer-tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "assert(isinstance(indexerMonth, StringIndexer))\nassert(isinstance(indexerCarrier, StringIndexer))\nassert(indexerMonth.getInputCol() == \"month\" and indexerMonth.getOutputCol() == \"monthIndexed\" and indexerMonth.getHandleInvalid() == \"keep\")\nassert(indexerCarrier.getInputCol() == \"carrier\" and indexerCarrier.getOutputCol() == \"carrierIndexed\" and indexerCarrier.getHandleInvalid() == \"keep\")"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "e6a7e3a800a782c19c16b753ef9cc6f2", "grade": false, "grade_id": "cell-e577747d4427e323", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Recordemos tambi\u00e9n que Spark requiere que todas las variables est\u00e9n en una \u00fanica columna de tipo vector, por lo que despu\u00e9s de indexar estas dos variables, tendremos que fusionar en una columna de tipo vector todas ellas, utilizando un `VectorAssembler`. Se pide:\n\n* **(1 punto)** Crear en una variable llamada `vectorAssembler` un `VectorAssembler` que reciba como entrada una lista de todas las variables de entrada (y que no debe incluir `arr_delay`) que ser\u00e1n las que formar\u00e1n parte del modelo. Crear primero esta lista de variables (lista de strings) en la variable `columnas_ensamblar` y pasar dicha variable como argumento al crear el `VectorAssembler`. Como es l\u00f3gico, en el caso de las columnas `month` y `carrier`, no usaremos las variables originales sino las indexadas en el apartado anterior. La columna de tipo vector creada con las caracter\u00edsticas ensambladas debe llamarse `features`."}, {"cell_type": "code", "execution_count": 18, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "44623471d64d0605fcbcd4bbcc3c2a0d", "grade": false, "grade_id": "vector-assembler", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Longitud Columnas Ensamblar:  7\n"}], "source": "# Incluye aqu\u00ed los imports que necesites\n# import ...........\n\nfrom pyspark.ml.feature import VectorAssembler\n\ncolumnas_ensamblar = [\"monthIndexed\",\"day\", \"dep_time\", \"arr_time\", \"carrierIndexed\", \"air_time\", \"distance\"]\nprint(\"Longitud Columnas Ensamblar: \" ,len(columnas_ensamblar))\noutputCol = \"features\"\nvectorAssembler = VectorAssembler(inputCols = columnas_ensamblar, outputCol=outputCol)\n\n# YOUR CODE HERE\n#raise NotImplementedError"}, {"cell_type": "code", "execution_count": 19, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "0d17b2fa1d8a4bd02b89952429ba1552", "grade": true, "grade_id": "vector-assembler-tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "assert(isinstance(vectorAssembler, VectorAssembler))\nassert(vectorAssembler.getOutputCol() == \"features\")\ninput_cols = vectorAssembler.getInputCols()\nassert(len(input_cols) == 7)\nassert(\"arr_delay\" not in input_cols)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "29cd94afed88265b627f01ac03361076", "grade": false, "grade_id": "cell-e577747d4427e32dsdf", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Finalmente, vemos que la columna `arr_delay` es continua, y no binaria como requiere un problema de clasificaci\u00f3n con dos clases. Vamos a convertirla en binaria. Para ello se pide:\n\n* **(0.5 puntos)** Utilizar un binarizador de Spark, fijando a 15 el umbral, y guardarlo en la variable `delayBinarizer`. Consideramos retrasado un vuelo que ha llegado con m\u00e1s de 15 minutos de retraso, y no retrasado en caso contrario. La nueva columna creada con la variable binaria debe llamarse `arr_delay_binary` y debe ser interpretada como la columna target para nuestro algoritmo. Por ese motivo, esta columna **no** se incluy\u00f3 en el apartado anterior entre las columnas que se ensamblan para formar las features."}, {"cell_type": "code", "execution_count": 20, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "1f33c493a3c3dd83fb6a39a7acefca5c", "grade": false, "grade_id": "binarizer", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# Incluye aqu\u00ed los imports que necesites y que no hayas incluido ya en alguna celda anterior\n# import .........\n\nfrom pyspark.ml.feature import Binarizer\n\ndelayBinarizer = Binarizer(inputCol=\"arr_delay\", outputCol=\"arr_delay_binary\", threshold=15.0)\n\n# YOUR CODE HERE\n#raise NotImplementedError"}, {"cell_type": "code", "execution_count": 21, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "63866fd5322ae78c26612ef7e1ccdeda", "grade": true, "grade_id": "binarizer-tests", "locked": true, "points": 0.5, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "assert(isinstance(delayBinarizer, Binarizer))\nassert(delayBinarizer.getThreshold() == 15)\nassert(delayBinarizer.getInputCol() == \"arr_delay\")\nassert(delayBinarizer.getOutputCol() == \"arr_delay_binary\")"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "d7dbd49d1ba9c1889183ea9d4ef4b1c7", "grade": false, "grade_id": "cell-25a7793978ee7d05", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(0.5 puntos)** Por \u00faltimo, crearemos el modelo de clasificaci\u00f3n.\n\n* Crear en una variable `decisionTree` un \u00e1rbol de clasificaci\u00f3n de Spark (`DecisionTreeClassifier` del paquete `pyspark.ml.classification`)\n* Indicar como columna de entrada la nueva columna creada por el `VectorAssembler` creado en un apartado anterior.\n* Indicar como columna objetivo (target) la nueva columna creada por el `Binarizer` del apartado anterior."}, {"cell_type": "code", "execution_count": 22, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "e785136d6d06691c003ff9542027e03d", "grade": false, "grade_id": "decision-tree", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# Incluye aqu\u00ed los imports que necesites y que no hayas incluido ya en alguna celda anterior\n# import .........\n\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\ndecisionTree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"arr_delay_binary\")\n\n# YOUR CODE HERE\n#raise NotImplementedError"}, {"cell_type": "code", "execution_count": 23, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "6551bea8cb96c1de22e91028295a3f6c", "grade": true, "grade_id": "decision-tree-tests", "locked": true, "points": 0.5, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "assert(isinstance(decisionTree, DecisionTreeClassifier))\nassert(decisionTree.getFeaturesCol() == \"features\")\nassert(decisionTree.getLabelCol() == \"arr_delay_binary\")"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "008efada185c29e19ebbe2b4a82216fd", "grade": false, "grade_id": "cell-e577747d4427e32d", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(1 punto)** Ahora vamos a encapsular todas las fases en un s\u00f3lo pipeline y procederemos a entrenarlo. Se pide:\n\n* Crear en una variable llamada `pipeline` un objeto `Pipeline` de Spark con las etapas anteriores en el orden adecuado para poder entrenar un modelo. \n\n* Entrenarlo invocando sobre ella al m\u00e9todo `fit` y guardar el pipeline entrenado devuelto por dicho m\u00e9todo en una variable llamada `pipelineModel`. \n\n* Aplicar el pipeline entrenado para transformar (predecir) el DataFrame `flightsConvertido`, guardando las predicciones devueltas en la variable `flightsPredictions` que ser\u00e1 un DataFrame. N\u00f3tese que estamos prediciendo los propios datos de entrenamiento y que, por simplicidad, no hab\u00edamos hecho (aunque habr\u00eda sido lo correcto) ninguna divisi\u00f3n de nuestros datos originales en subconjuntos distintos de entrenamiento y test antes de entrenar."}, {"cell_type": "code", "execution_count": 24, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "edbdb627305d03efa41a88426330e160", "grade": false, "grade_id": "pipeline", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Incluye aqu\u00ed los imports que necesites y que no hayas incluido ya en alguna celda anterior\n# import .........\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\n\npipeline = Pipeline(stages=[indexerMonth,indexerCarrier, vectorAssembler, delayBinarizer, decisionTree])\npipelineModel = pipeline.fit(flightsConvertido)\nflightsPredictions = pipelineModel.transform(flightsConvertido)\n\n# YOUR CODE HERE\n#raise NotImplementedError"}, {"cell_type": "code", "execution_count": 25, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "befe2447f17ba2d8174aee074901c82a", "grade": true, "grade_id": "pipeline-tests", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "from pyspark.ml import PipelineModel\nassert(isinstance(pipeline, Pipeline))\nassert(len(pipeline.getStages()) == 5)\nassert(isinstance(pipelineModel, PipelineModel))\nassert(\"probability\" in flightsPredictions.columns)\nassert(\"prediction\" in flightsPredictions.columns)\nassert(\"rawPrediction\" in flightsPredictions.columns)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "0c531953abf18cfb3b67571ddde7a57d", "grade": false, "grade_id": "cell-61156fe5938763f1", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Vamos a mostrar la matriz de confusi\u00f3n (este apartado no es evaluable). Agrupamos por la variable que tiene la clase verdadera y la que tiene la clase predicha, para ver en cu\u00e1ntos casos coinciden y en cu\u00e1ntos difieren."}, {"cell_type": "code", "execution_count": 26, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "75f98ae39b827e75c3f0b4b2aaa6b0db", "grade": false, "grade_id": "cell-896752beb71cb455", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 60:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------------+----------+------+\n|arr_delay_binary|prediction| count|\n+----------------+----------+------+\n|             1.0|       1.0|   505|\n|             0.0|       1.0|    73|\n|             1.0|       0.0| 23744|\n|             0.0|       0.0|136426|\n+----------------+----------+------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "flightsPredictions.groupBy(\"arr_delay_binary\", \"prediction\").count().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}