{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "5429c6d22780204b04cd76fee600744f", "grade": false, "grade_id": "cell-20cd3bb8f5a12f07", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# Actividad 2: Structured Streaming y Kafka\n\n## Actividad NO perteneciente al Proyecto (actividad individual)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Recuerda borrar siempre las l\u00edneas que dicen `raise NotImplementedError`"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "247b97047f5d8dd085c08e819245124d", "grade": false, "grade_id": "cell-2fbae7fd82212064", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "## IMPORTANTE: no se corregir\u00e1 ning\u00fan notebook que no est\u00e9 completamente ejecutado y muestre todas las salidas\n\n### Punto de partida (final de la actividad 1): funci\u00f3n `retraso_medio_periodo` modificada\n#### Este ejercicio no suma puntos pero es necesario para el resto del notebook\n\nRecordatorio: *El c\u00f3digo que calcule esto deber\u00eda ir encapsulado en una funci\u00f3n de Python que reciba como argumento un DataFrame y devuelva como resultado el DataFrame con el c\u00e1lculo indicado en el ejercicio 2 de la Actividad 1 de la asignatura*\n\nCopia en la siguiente celda el c\u00f3digo de tu funci\u00f3n `retraso_medio_periodo` que has completado en el ejercicio 2 de la Actividad 1, y modifica el c\u00f3digo de la siguiente manera.\n\n* **Elimina el pivot**, y modifica el groupBy para incluir en \u00e9l la columna `periodo_dia` por la que hab\u00edas pivotado. Es decir, las dos columnas agrupadoras son `dest` y `periodo_dia`.\n* Nombra la columna de agregaci\u00f3n con el alias `avg_delay`.\n* A\u00f1ade dentro de la agregaci\u00f3n otra columna adicional llamada `n_vuelos` que sea el total de filas que forman cada grupo.\n* **Modifica la columna `alumno` para poner tu nombre y primer apellido (elimina los apellidos de tus compa\u00f1eros de grupo de la Actividad 1)**. Puedes usar withColumn y dentro de ella, el segundo argumento ser\u00e1 `F.lit(\"nombre apellido\")`.\n* Adem\u00e1s, a\u00f1ade al DF **una nueva columna llamada `timestamp`** de valor igual a `F.current_timestamp()` (no es necesario utilizar `F.lit`). Nos servir\u00e1 m\u00e1s adelante para ver en cu\u00e1ntos batches ha procesado Spark la informaci\u00f3n."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "ea274c76955fa5374c67cd75eaeea696", "grade": false, "grade_id": "retraso-medio", "locked": false, "schema_version": 3, "solution": true, "task": false}, "tags": []}, "outputs": [], "source": "from pyspark.sql import DataFrame\n# no olvides otros import que te hagan falta\n\ndef retraso_medio_periodo(df: DataFrame) -> DataFrame:\n    # Pega aqu\u00ed el mismo c\u00f3digo que escribiste en el ejercicio 2 de la Actividad 1,\n    # modificando la columna alumno para que contenga solamente tu nombre y primer apellido, en lugar de los apellidos del grupo.\n    return None\n\n# YOUR CODE HERE\nraise NotImplementedError"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "dd5d662b5807f8a8e414cb38a911d513", "grade": true, "grade_id": "retraso-medio-test", "locked": true, "points": 0, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "df_prueba = spark.createDataFrame([(\"GRX\", 1330.0, 23.0)], [\"dest\", \"dep_time\", \"arr_delay\"])\ndf_retraso = retraso_medio_periodo(df_prueba)\nr = df_retraso.first()\nassert(len(df_retraso.columns) == 6)\nassert(all([x in df_retraso.columns for x in [\"dest\", \"periodo_dia\", \"avg_delay\", \"n_vuelos\", \"alumno\", \"timestamp\"]])) # comprobamos las columnas que debe tener\nassert(r.dest == \"GRX\" and r.periodo_dia == \"mediodia\" and r.avg_delay == 23.0 and r.n_vuelos == 1 and r.alumno is not None and r.timestamp is not None)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Instalaci\u00f3n del paquete confluent-kafka para interactuar con Kafka desde Python sin usar Spark"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!pip install confluent-kafka"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "3ece157850ba17e5f11c59b897710f6c", "grade": false, "grade_id": "cell-2893d926b52cfcdd", "locked": true, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "source": "## Funciones auxiliares para crear y borrar el topic revisiones (completa el c\u00f3digo con el nombre de tu cluster)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from confluent_kafka import Producer\nfrom confluent_kafka.admin import AdminClient, NewTopic\nimport socket\n\ndef crear_topic(admin_client: AdminClient, topic_name: str):\n    \"\"\"\n    Crea el topic llamado topic_name con una sola partici\u00f3n y factor replicaci\u00f3n 1.\n    No devuelve nada pero mostrar\u00e1 un mensaje indicando si ha ido bien o no.\n    \n    :param admin_client: objeto AdminClient con el cliente de Kafka ya configurado\n    :param topic_name: string con el nombre del topic que deseamos crear\n    \"\"\"\n    topic = NewTopic(topic_name, num_partitions=1, replication_factor=1)\n    fs = admin_client.create_topics([topic])\n    for topic, f in fs.items():\n        try:\n            f.result()\n            print(f\"Topic {topic_name} creado\")\n        except Exception as e:\n            print(f\"Fallo al crear el topic {topic_name}: {e}\")\n\ndef borrar_topic(admin_client: AdminClient, topic_name: str):\n    \"\"\"\n    Borra un topic existente llamado topic_name que debe existir.\n    No devuelve nada pero mostrar\u00e1 un mensaje indicando si ha ido bien o no.\n    \n    :param admin_client: objeto AdminClient con el cliente de Kafka ya configurado\n    :param topic_name: string con el nombre del topic que deseamos borrar\n    \"\"\"\n    fs = admin_client.delete_topics([topic_name])\n    for topic, f in fs.items():\n        try:\n            f.result()\n            print(f\"Topic {topic_name} borrado\")\n        except Exception as e:\n            print(f\"Fallo al borrar el topic {topic_name}: {e}\")"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## Cliente para interactuar con Kafka mediante el paquete confluent-kafka, sin usar Spark"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "53222efa607c090508c70d3c1fccee87", "grade": true, "grade_id": "crear-cliente-kafka", "locked": false, "points": 0, "schema_version": 3, "solution": true, "task": false}, "tags": []}, "outputs": [], "source": "nombre_cluster = None    # COMPLETA ESTA L\u00cdNEA\n\n# DESCOMENTA ESTAS L\u00cdNEAS PARA CREAR EL CLIENTE CON EL NOMBRE DE TU CLUSTER\n\n# conf = {\"bootstrap.servers\": f\"{nombre_cluster}-w-0:9092,{nombre_cluster}-w-1:9092\",\n#         \"sasl.mechanism\": \"PLAIN\",\n#         \"client.id\": socket.gethostname()}\n\n# producer = Producer(conf)\n# admin_client = AdminClient(conf)\n\n# YOUR CODE HERE\nraise NotImplementedError"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "71af96b0ca9bd0408130334c2be4b95e", "grade": false, "grade_id": "cell-b302ba8d137a4212", "locked": true, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "source": "### Funciones para borrar el topic, si algo hubiera ido mal y queremos repetir, y para crearlo tras haberlo borrado (o la primera vez)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# borrar_topic(admin_client, \"retrasos\")\ncrear_topic(admin_client, \"retrasos\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## Funci\u00f3n para insertar datos en Kafka, en lotes de 10.000 cada 2 segundos, para dar tiempo a visualizar el resultado cambiante"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "import os\nfrom concurrent.futures import ThreadPoolExecutor\nimport pandas as pd\n\nexecutor = ThreadPoolExecutor(max_workers=1)\n\ndef insertar_kafka(producer: Producer, ruta_csv: str):\n    \"\"\"\n    Inserta en Kafka los mensajes de las filas del fichero CSV en ruta_csv,\n    escogiendo solo las columnas dest, dep_time y arr_delay. Cada fila es\n    un mensaje formateado como JSON. \n    \"\"\"\n\n    if not os.path.isfile(\"flights.csv\"):\n        print(\"Trayendo flights.csv desde el bucket de Google Cloud Storage ...\")\n        os.system(f\"hdfs dfs -copyToLocal -f {ruta_csv} .\")\n    \n    print(\"Leyendo datos del CSV ...\")\n    vuelos_df = pd.read_csv(\"flights.csv\")\n    \n    # Formateamos los mensajes como cadenas de texto con estructura de JSON con los campos dest, dep_time y arr_delay\n    vuelos_dict = vuelos_df.loc[:, [\"dest\", \"dep_time\", \"arr_delay\"]].transpose().to_dict()\n    mensajes = [str(vuelos_dict[i]).replace(\"'\", \"\\\"\") for i in range(len(vuelos_dict))]\n    \n    print(\"Insertando mensajes en el topic retrasos ...\")\n    \n    def insertar_bucle():\n        import time\n        for i, msj in enumerate(mensajes):\n            producer.produce(\"retrasos\", value=msj)\n            if i % 10000 == 0:\n                print(f\"Se ha insertado el mensaje {i} con valor {msj}\")\n                producer.flush()\n                time.sleep(5)\n    \n    # Ejecutamos de manera as\u00edncrona todo el bucle de inserciones, para simult\u00e1neamente poder \n    # ejecutar la otra celda en la que hacemos show de la tabla de salida donde escribe Spark Structured Streaming\n    resultado = executor.submit(insertar_bucle)\n    return resultado"}, {"cell_type": "markdown", "metadata": {}, "source": "### Ejercicio 1\n\nUtilizaremos Kafka para actualizar en tiempo real el resultado calculado en el apartado anterior.\n\nPara simplificar, asumimos que los mensajes le\u00eddos de Kafka tiene solamente tres campos que son los \u00fanicos necesarios para llevar a cabo la operaci\u00f3n anterior: dest, dep_time y arr_delay. La idea ser\u00e1 crear un Streaming DataFrame para leer de Kafka, y despu\u00e9s invocar a nuestra funci\u00f3n `retraso_medio_periodo`, pas\u00e1ndolo dicho DF como argumento. Vamos a leer del topic `retrasos`, por lo que debes indicar esta opci\u00f3n a continuaci\u00f3n.\n\nSe pide crear, en la variable `retrasosStreamingDF`, un Streaming DataFrame leyendo de Apache Kafka, configurando las siguientes opciones:\n  * Usar la variable `readStream` (en lugar de `read` como solemos hacer) interna de la SparkSession `spark`\n  * Indicar que el formato es `\"kafka\"` con `.format(\"kafka\")`\n  * Indicar cu\u00e1les son los brokers de Kafka de los que vamos a leer y el puerto al que queremos conectarnos para leer (9092 es el que usa Kafka por defecto), con `.option(\"kafka.bootstrap.servers\", \"nombre_cluster-w-0:9092,nombre_cluster-w-1:9092\")`. De esa manera podremos leer el mensaje si el productor de Kafka lo env\u00eda a cualquiera de los dos brokers existentes, que son los nodos del cluster identificados como `nombre_cluster-w-0` y `nombre_cluster-w-1`\n  * Indicar que queremos subscribirnos al topic `\"retrasos\"` con `.option(\"subscribe\", \"retrasos\")`.\n  * Finalmente ponemos `load()` para realizar la lectura."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "7a43384ae5c02d890f68c28351a6d824", "grade": false, "grade_id": "read-stream", "locked": false, "schema_version": 3, "solution": true, "task": false}, "tags": []}, "outputs": [], "source": "# Esto hace m\u00e1s eficientes las transformaciones wide con vol\u00famenes peque\u00f1os de datos\nspark.conf.set(\"spark.sql.shuffle.partitions\", 8)\nspark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n\n# Reemplaza por el c\u00f3digo correcto siguiendo las indicaciones anteriores\nretrasosStreamingDF = None \n\n# YOUR CODE HERE\nraise NotImplementedError"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "eb5c34086b3c3fb29a4c46865396bcb1", "grade": true, "grade_id": "read-stream-tests", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "# Mostramos el esquema de este DataFrame\ntypes = retrasosStreamingDF.dtypes\nassert(retrasosStreamingDF.isStreaming)\nassert((types[0][0] == \"key\")       & (types[0][1] == \"binary\"))\nassert((types[1][0] == \"value\")     & (types[1][1] == \"binary\"))\nassert((types[2][0] == \"topic\")     & (types[2][1] == \"string\"))\nassert((types[3][0] == \"partition\") & (types[3][1] == \"int\"))\nassert((types[4][0] == \"offset\")    & (types[4][1] == \"bigint\"))\nassert((types[5][0] == \"timestamp\") & (types[5][1] == \"timestamp\"))\nassert((types[6][0] == \"timestampType\") & (types[6][1] == \"int\"))"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "ef304d531ef4b7375c21516e7c7bf897", "grade": false, "grade_id": "cell-580bf3caf39b314e", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "Muestra por pantalla el esquema del DataFrame resultante de la lectura con `printSchema()`. Ver\u00e1s que todas estas columnas son creadas autom\u00e1ticamente por Spark cuando leemos de Kafka. De ellas, la que nos interesa es `value` que contiene propiamente el mensaje de Kafka, en formato datos binarios. \n\n### Ejercicio 2\n\nTendremos que estructurar los datos le\u00eddos desde Kafka para poder extraer los campos. Para ello sigue los siguientes pasos, ayud\u00e1ndote de la plantilla que hay en la celda siguiente (descom\u00e9ntala y compl\u00e9tala):\n\n* **Selecciona** la columna `value` y convi\u00e9rtela (`.cast`) a `StringType()` utilizando `withColumn` para reemplazar la columna existente `\"value\"` por el objeto Column resultante de la conversi\u00f3n. De esta forma tendremos una columna que contendr\u00e1 en cada **fila** un **fichero JSON completo**, tal como se muestra en cada una de las plantillas anteriores. \n* Para extraer los dos campos de cada uno de los JSON y convertirlos en una columna llamada `tuplas`, de tipo `struct` (una estructura formada por varios campos de tipos String o Double, seg\u00fan el campo), utilizaremos la funci\u00f3n `from_json` de Spark, que se aplica a cada elemento (cada fila) de la columna \"value\" y parsea el String seg\u00fan un esquema que le indiquemos, devolviendo una columna de tipo `struct`.\n* La columna `tuplas` es de tipo `struct` por lo que puedes acceder a cada uno de sus campos (`dest`, `dep_time` y `arr_delay`) con el operador `.` (punto). Utilizando `withColumn` dos veces, crea tres columnas llamadas `dest`, `dep_time` y `arr_delay` como el resultado de acceder a `tuplas.dest`, `tuplas.dep_time` y `tuplas.arr_delay` respectivamente."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "192841dd490e77a7376bab23fcf0ac0a", "grade": false, "grade_id": "estructura-json", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\nesquema = StructType([\n  StructField(\"dest\", StringType()),\n  StructField(\"dep_time\", DoubleType()),\n  StructField(\"arr_delay\", DoubleType())\n])\n\nparsedDF = None   # borra esta l\u00ednea y reemplaza por las siguientes l\u00edneas (descom\u00e9ntalas y compl\u00e9talas)\n\n# parsedDF = <COMPLETAR>\\\n#     .select(<COMPLETAR)\\\n#     .<COMPLETAR>(\"value\", F.col(\"value\").cast(<COMPLETAR>))\\\n#     .withColumn(<COMPLETAR>, F.from_json(F.col(\"value\"), esquema))\\\n#     .withColumn(<COMPLETAR>, F.col(\"tuplas.dest\"))\\\n#     .withColumn(<COMPLETAR>, F.col(\"tuplas.dep_time\"))\\\n#     .withColumn(<COMPLETAR>, F.col(\"tuplas.arr_delay\"))\\\n\n# YOUR CODE HERE\nraise NotImplementedError"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "d03113f6b23f47c48b21e4f449cc83ec", "grade": true, "grade_id": "estructura-json-tests", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "tipos = parsedDF.dtypes\nassert((\"value\", \"string\") in tipos)\nassert(('tuplas', 'struct<dest:string,dep_time:double,arr_delay:double>') in tipos)\nassert(('dest', 'string') in tipos)\nassert(('dep_time', 'double') in tipos)\nassert(('arr_delay', 'double') in tipos)"}, {"cell_type": "markdown", "metadata": {}, "source": "* Nuestro DataFrame ya contiene una columna `dest` con el nombre del aeropuerto destino, una columna `dep_time` con la hora de salida, y una columna de n\u00fameros reales `arr_delay` con el retraso. \n* Por tanto, ya podemos efectuar el mismo tipo de agregaci\u00f3n que estamos haciendo en nuestra funci\u00f3n `retraso_medio_periodo`. \n* Para ello, invocamos a `retraso_medio_periodo` pasando como argumento `parsedDF`, que es un Streaming DataFrame porque proviene de transformar `retrasosStreamingDF` que tambi\u00e9n es un Streaming DataFrame."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Eval\u00faa el siguiente c\u00f3digo pero no lo modifiques\n# Indicamos que este DataFrame se guarde en memoria cuando se va actualizando,\n# y arrancamos la ejecuci\u00f3n en Streaming con la acci\u00f3n start()\n\nretraso_medio_streaming_df = retraso_medio_periodo(parsedDF)\n\nconsoleOutput = retraso_medio_streaming_df\\\n                    .writeStream\\\n                    .queryName(\"retrasosAgg\")\\\n                    .outputMode(\"complete\")\\\n                    .format(\"memory\")\\\n                    .start()\n"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "42e0b3ed1f04aed5fee37851c2db332d", "grade": false, "grade_id": "cell-b073802f3eec8bb3", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "## Ejercicio 3: inserci\u00f3n en Kafka + lectura desde Kafka con Spark y agregaci\u00f3n en tiempo real\n\nUna vez evaluada la celda anterior, ejecuta la celda siguiente (funci\u00f3n `insertar_kafka`), y **cuando empiecen a aparecer mensajes indicando que se empiezan a insertar mensajes en Kafka**, entonces ejecuta (sin esperar m\u00e1s) la celda posterior, que contiene un bucle.\n* La funci\u00f3n `insertar_kafka`, por dentro, descargar\u00e1 el fichero flights.csv desde GCS al disco duro de la m\u00e1quina nombrecluster-m y, despu\u00e9s de esto, se pondr\u00e1 en segundo plano a insertar mensajes en Kafka. Esto \u00faltimo lo har\u00e1 sin bloquear el notebook, por lo que, llegado ese momento, ya podr\u00e1s ejecutar la celda posterior.\n* La segunda celda contiene **un c\u00f3digo que debes completar** para leer el contenido de la vista temporal `retrasosAgg`, quedarte s\u00f3lo con los vuelos cuyo destino es `JFK`, e imprimir dichas filas. Ver\u00e1s que, en cada iteraci\u00f3n del bucle que contiene la celda, el resultado va cambiando, debido a que cada vez hay m\u00e1s vuelos en Kafka, y esto provoca que el retraso medio en cada franja del d\u00eda se vaya actualizando.\n* Es posible que, en algunas iteraciones, el contenido no haya cambiado con respecto a la salida previa porque no se ha llegado a refrescar. No importa. Debes asegurarte, en cualquier caso, de mostrar la salida final, que nunca cambia, lo que significa que ya se han insertado todos los mensajes en Kafka, y que Spark ya ha calculado la versi\u00f3n m\u00e1s reciente del resultado. Como no se van a insertar m\u00e1s mensajes una vez que se han insertado todas las filas del CSV como mensajes, el resultado de la agregaci\u00f3n ya no volver\u00e1 a cambiar.\n* El test de auto-evaluaci\u00f3n comprueba precisamente los valores finales, asumiendo que ya se han insertado todos los datos en Kafka (una sola vez).\n  * Si algo va mal, no vuelvas a insertar mensajes en el topic porque entonces, el resultado final de la agregaci\u00f3n saldr\u00e1 incorrecto. Si necesitas repetir la ejecuci\u00f3n, aseg\u00farate de ejecutar la \u00faltima celda de este notebook antes de volver a insertar nada en Kafka.\n* **No se puntuar\u00e1 el ejercicio si no se ha imprimido por pantalla el \u00faltimo resultado (las 4 filas del JFK) donde se vea que los valores mostrados en pantalla coinciden con los de la celda de autoevaluaci\u00f3n**.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "20f6954759efe23be0b9708ddcb33f0e", "grade": false, "grade_id": "insertar-kafka", "locked": false, "schema_version": 3, "solution": true, "task": false}, "tags": []}, "outputs": [], "source": "import time\n\nruta_flights_gcs = None   # ruta del fichero flights.csv en Google Cloud Storage\n\n# resultado = insertar_kafka(producer, ruta_flights_gcs)\n\n# YOUR CODE HERE\nraise NotImplementedError"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "5e0f3d1d09a7fe78227cbdfb2454acbc", "grade": false, "grade_id": "sql-streaming", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# Modifica esta l\u00ednea para invocar al m\u00e9todo .sql() de la spark session, \n# en el cual debes leer de retrasosAgg y quedarte solamente con los que tienen destino JFK\n\nagregadosDF = None   \n\n# DESCOMENTAR EL SIGUIENTE C\u00d3DIGO TRAS HABER RESUELTO LA L\u00cdNEA ANTERIOR\n\n# for i in range(15):\n#     time.sleep(7)        # queremos dar tiempo a ir calculando resultados en un cluster tan peque\u00f1o\n#     agregadosDF.show(truncate=False)\n\n# Debes poder ver varias salidas distintas. Quiz\u00e1 no todas las 15 impresiones sean diferentes, pero muchas de ellas s\u00ed.\n# Esto te muestra c\u00f3mo Structured Streaming est\u00e1 actualizando el resultado cada vez. Probablemente las 3 o 4 \u00faltimas se\n# repetir\u00e1n porque ya estar\u00e1n mostrando el resultado final, que no var\u00eda.\n\n# YOUR CODE HERE\nraise NotImplementedError"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "5e7ecb0d4027581deda9b0ff63775780", "grade": true, "grade_id": "sql-streaming-test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "columnas = agregadosDF.columns\nassert(len(columnas) == 6)\nassert(all([x in [\"dest\", \"periodo_dia\", \"avg_delay\", \"n_vuelos\", \"alumno\", \"timestamp\"] for x in columnas])) # comprobamos las columnas que debe tener agregadosDF\nlista = agregadosDF.where(\"dest = 'JFK'\").sort(\"dest\").collect()\nassert(lista[0].periodo_dia == 'maniana' and round(lista[0].avg_delay, 2) == 28.35 and lista[0].n_vuelos==356)\nassert(lista[1].periodo_dia == 'tarde' and round(lista[1].avg_delay, 2) == 52.5 and lista[1].n_vuelos==8)\nassert(lista[2].periodo_dia == 'noche' and round(lista[2].avg_delay, 2) == 26.82 and lista[2].n_vuelos==620)\nassert(lista[3].periodo_dia == 'mediodia' and round(lista[3].avg_delay, 2) == 42.11 and lista[3].n_vuelos==333)"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "# Ejercicio 4: c\u00f3mo se actualiza el resultado al a\u00f1adir manualmente un dato m\u00e1s a Kafka\n\nVamos a insertar manualmente un nuevo mensaje en Kafka mediante el Kafka Console Producer, que puedes abrir ejecutando en una terminal (sin salir de JupyterLab) el siguiente comando. La terminal que se abre enviar\u00e1 datos al topic retrasos. Cada dato debes escribirlo en una l\u00ednea y pulsar ENTER para que se env\u00ede al topic retrasos.\n\n`/usr/lib/kafka/bin/kafka-console-producer.sh --broker-list nombrecluster-w-0:9092 --topic retrasos` \n\n(**modifica el nombre del cluster para poner el tuyo**)\n\nEnv\u00eda el siguiente mensaje a Kafka, copi\u00e1ndolo (Shift + Click derecho-copiar) y peg\u00e1ndolo (Shift + click derecho-pegar) en el Kafka Console Producer:\n\n`{\"dest\": \"JFK\", \"dep_time\": 1800.0, \"arr_delay\": 75.0}`\n\nComo es un vuelo de las 18:00 horas, pertenece al per\u00edodo \"tarde\", por lo que ver\u00e1s que aumenta el n_vuelos, pasando de 8 a 9 y el retraso medio tambi\u00e9n, puesto que ha llegado con 75 minutos de retraso frente a los 52.5 minutos que hab\u00eda calculados hasta ahora de media."}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "da06d09c9f243aaff16c57096fccd3e5", "grade": false, "grade_id": "mensaje-extra", "locked": false, "schema_version": 3, "solution": true, "task": false}, "tags": []}, "outputs": [], "source": "# Ejecuta varias veces esta celda hasta ver que ha cambiado el DF resultante\nagregadosDF.show(truncate=False)\n# YOUR CODE HERE\nraise NotImplementedError"}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "77cce2ca8f84e69d6fedb8a02a9da941", "grade": true, "grade_id": "mensaje-extra-test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "outputs": [], "source": "lista = agregadosDF.collect()\nassert(lista[1].periodo_dia == 'tarde' and round(lista[1].avg_delay, 2) == 55.0 and lista[1].n_vuelos==9)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Si algo va mal y hay que repetir... ejecuta la celda siguiente\n\n* Det\u00e9n el flujo de procesamiento con `consoleOutput.stop()`\n* Borra el topic `retrasos` de Kafka (puede ser necesario ejecutar varias veces la sentencia de borrado) y vu\u00e9lvelo a crear.\n* Borra la vista temporal `retrasosAgg`\n* Vuelve a ejecutar la celda de `consoleOutput = retraso_medio_streaming_df.writeStream()...start()`\n* Vuelve a ejecutar la celda de inserci\u00f3n en Kafka y el bucle de escrituras por pantalla."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import time\n\nconsoleOutput.stop()\nprint(\"Consulta finalizada\")\n\nspark.sql(\"drop table retrasosAgg\")\nprint(\"Vista retrasosAgg borrada\")\n\nborrar_topic(admin_client, \"retrasos\")\ntime.sleep(3)\ncrear_topic(admin_client, \"retrasos\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## Si aun as\u00ed, no se arregla ...\n\n* Salva el notebook (bot\u00f3n del diskette, o File -> Save Notebook)\n* En el men\u00fa de la izquierda (icono de un c\u00edrculo negro con un cuadrado blanco dentro, Running Terminals and Kernels) -> Kernels -> Shut Down All\n* Cierra la pesta\u00f1a del notebook\n* Vuelve al explorador del men\u00fa de la izquierda (icono de la carpeta) -> abre de nuevo tu notebook en tu workspace, y selecciona Kernel -> PySpark (esquina superior derecha)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}